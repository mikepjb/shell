models:
  qwen3-coder-30b-a3b:
    source:
      repo: "unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"
      file: "Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL.gguf"
    context: 32768
    temp: 0.7
    top-p: 0.8
    top-k: 20
    repeat-penalty: 1.05
    port: 9091
  devstral-small-2:
    source:
      repo: "bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF"
      file: "mistralai_Devstral-Small-2-24B-Instruct-2512-Q4_K_M.gguf"
    context: 32768
    temp: 0.7
    top-p: 0.8
    top-k: 20
    repeat-penalty: 1.05
    port: 9092
  qwen3-4b:
    source:
      repo: "unsloth/Qwen3-4B-Instruct-2507-GGUF"
      file: "Qwen3-4B-Instruct-2507-Q4_K_M.gguf"
    context: 32768
    gpu-layers: 99
    temp: 0.7
    top-p: 0.8
    top-k: 20
    repeat-penalty: 1.05
    port: 9093
  gemma-3-4b:
    source:
      repo: "bartowski/google_gemma-3-4b-it-GGUF"
      file: "google_gemma-3-4b-it-Q4_K_M.gguf"
    context: 32768
    gpu-layers: 99
    temp: 0.7
    top-p: 0.8
    top-k: 20
    repeat-penalty: 1.05
    port: 9094
  phi-4-mini:
    source:
      repo: "bartowski/microsoft_Phi-4-mini-instruct-GGUF"
      file: "microsoft_Phi-4-mini-instruct-Q4_K_M.gguf"
    context: 16384
    gpu-layers: 99
    temp: 0.7
    top-p: 0.8
    top-k: 20
    repeat-penalty: 1.05
    port: 9095
  llama-3.2-3b:
    source:
      repo: "bartowski/Llama-3.2-3B-Instruct-GGUF"
      file: "Llama-3.2-3B-Instruct-Q4_K_M.gguf"
    context: 32768
    gpu-layers: 99
    temp: 0.6
    top-p: 0.85
    top-k: 30
    repeat-penalty: 1.05
    port: 9096
  llama-3.2-1b:
    source:
      repo: "bartowski/Llama-3.2-1B-Instruct-GGUF"
      file: "Llama-3.2-1B-Instruct-Q4_K_M.gguf"
    context: 32768
    gpu-layers: 99
    temp: 0.6
    top-p: 0.85
    top-k: 30
    repeat-penalty: 1.05
    port: 9097
  qwen3-0.6b:
    source:
      repo: "unsloth/Qwen3-0.6B-GGUF"
      file: "Qwen3-0.6B-Q8_0.gguf"
    context: 16384
    gpu-layers: 99
    temp: 0.7
    top-p: 0.8
    top-k: 20
    repeat-penalty: 1.05
    port: 9098
  smollm2-1.7b:
    source:
      repo: "HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF"
      file: "smollm2-1.7b-instruct-q4_k_m.gguf"
    context: 8192
    gpu-layers: 99
    temp: 0.7
    top-p: 0.8
    top-k: 20
    repeat-penalty: 1.05
    port: 9099
